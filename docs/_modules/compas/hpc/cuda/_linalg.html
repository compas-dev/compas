
<!DOCTYPE html>

<html lang="en">
<head>
<title>compas -- a computational framework for research in architecture and structures</title>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="Tom Van Mele" name="author"/>
<meta content="compas is a computational framework for research in architecture and structures." name="description"/>
<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" rel="stylesheet"/>
<link href="/css/github.css" rel="stylesheet" type="text/css"/>
<link href="/css/compas.css" rel="stylesheet" type="text/css"/>
<link href="/css/compas-reference.css" rel="stylesheet" type="text/css"/>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
</head>
<body data-spy="scroll" data-target="#compas-localnav">
<header class="navbar navbar-expand navbar-dark bg-dark compas-navbar">
<a class="navbar-brand" href="/">compas</a>
<ul class="navbar-nav">
<li class="nav-item">
<a class="nav-link active" href="/compas">Main library</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/packages">Additional Packages</a>
</li>
</ul>
</header>
<div class="container-fluid compas-container">
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 compas-content" role="main">
<nav class="breadcrumb">
<a class="breadcrumb-item" href="https://compas-dev.github.io/">compas</a>
<a class="breadcrumb-item" href="https://compas-dev.github.io/compas/index.html">main library</a>
<a accesskey="U" class="breadcrumb-item" href="../../../index.html">module code</a>
</nav>
<h1>Source code for compas.hpc.cuda._linalg</h1><div class="highlight"><pre><code class="language-python border rounded">
<span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">float64</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">ceil</span>

<span class="kn">from</span> <span class="nn">compas.hpc.cuda._math</span> <span class="k">import</span> <span class="n">cuda_sqrt</span>
<span class="kn">from</span> <span class="nn">compas.hpc.cuda._math</span> <span class="k">import</span> <span class="n">cuda_sum</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">pycuda</span>
    <span class="kn">import</span> <span class="nn">pycuda.autoinit</span>
<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">skcuda</span>
    <span class="kn">import</span> <span class="nn">skcuda.linalg</span>
<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">pass</span>


<span class="n">__author__</span>     <span class="o">=</span> <span class="p">[</span><span class="s1">'Andrew Liew &lt;liew@arch.ethz.ch&gt;'</span><span class="p">]</span>
<span class="n">__copyright__</span>  <span class="o">=</span> <span class="s1">'Copyright 2016, Block Research Group - ETH Zurich'</span>
<span class="n">__license__</span>    <span class="o">=</span> <span class="s1">'MIT License'</span>
<span class="n">__email__</span>      <span class="o">=</span> <span class="s1">'liew@arch.ethz.ch'</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">'cuda_conj'</span><span class="p">,</span>
    <span class="s1">'cuda_cross'</span><span class="p">,</span>
    <span class="s1">'cuda_det'</span><span class="p">,</span>
    <span class="s1">'cuda_dot'</span><span class="p">,</span>
    <span class="s1">'cuda_eig'</span><span class="p">,</span>
    <span class="s1">'cuda_hermitian'</span><span class="p">,</span>
    <span class="s1">'cuda_inv'</span><span class="p">,</span>
    <span class="s1">'cuda_normrow'</span><span class="p">,</span>
    <span class="s1">'cuda_pinv'</span><span class="p">,</span>
    <span class="s1">'cuda_svd'</span><span class="p">,</span>
    <span class="s1">'cuda_trace'</span><span class="p">,</span>
    <span class="s1">'cuda_transpose'</span>
<span class="p">]</span>


<div class="viewcode-block" id="cuda_conj"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_conj.html#compas.hpc.cuda_conj">[docs]</a><span class="k">def</span> <span class="nf">cuda_conj</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Complex conjugate of GPUArray elements.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): Complex GPUArray.</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: The complex conjugate of the GPUArray.</span>

<span class="sd">    Examples:</span>
<span class="sd">        a = cuda_conj(cuda_give([1 + 2.j, 3 - 4.j], type='complex'))</span>
<span class="sd">        array([ 1.-2.j,  3.+4.j], dtype=complex64)</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<span class="k">try</span><span class="p">:</span>
    <span class="n">kernel_code_template</span> <span class="o">=</span> <span class="s2">"""</span>
<span class="s2">    __global__ void cross_product(float *a, float *b, float *c)</span>
<span class="s2">    {</span>
<span class="s2">        int i = threadIdx.x + blockDim.x * blockIdx.x;</span>
<span class="s2">        int n = 3;</span>
<span class="s2">        float A1 = a[i * n + 0];</span>
<span class="s2">        float A2 = a[i * n + 1];</span>
<span class="s2">        float A3 = a[i * n + 2];</span>
<span class="s2">        float B1 = b[i * n + 0];</span>
<span class="s2">        float B2 = b[i * n + 1];</span>
<span class="s2">        float B3 = b[i * n + 2];</span>
<span class="s2">        c[i * n + 0] = A2 * B3 - A3 * B2;</span>
<span class="s2">        c[i * n + 1] = A3 * B1 - A1 * B3;</span>
<span class="s2">        c[i * n + 2] = A1 * B2 - A2 * B1;</span>
<span class="s2">    }</span>
<span class="s2">    """</span>
    <span class="n">kernel_code</span> <span class="o">=</span> <span class="n">kernel_code_template</span> <span class="o">%</span> <span class="p">{</span><span class="s1">'m'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">pycuda</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">SourceModule</span><span class="p">(</span><span class="n">kernel_code</span><span class="p">)</span>
    <span class="n">cross_product</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">get_function</span><span class="p">(</span><span class="s2">"cross_product"</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">NameError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">pass</span>


<div class="viewcode-block" id="cuda_cross"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_cross.html#compas.hpc.cuda_cross">[docs]</a><span class="k">def</span> <span class="nf">cuda_cross</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">bsize</span><span class="p">):</span>
    <span class="sd">""" Cross-product of two GPUArrays (row by row).</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray 1 of vectors (m x 3).</span>
<span class="sd">        b (gpu): GPUArray 2 of vectors (m x 3).</span>
<span class="sd">        bsize (int): &lt; Blocksize divided by 3.</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Returns the m vectors from a x b</span>
<span class="sd">    """</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pycuda</span><span class="o">.</span><span class="n">gpuarray</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">float64</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">ceil</span><span class="p">(</span><span class="n">m</span> <span class="o">/</span> <span class="n">bsize</span><span class="p">)),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cross_product</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">block</span><span class="o">=</span><span class="p">(</span><span class="n">bsize</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">grid</span><span class="o">=</span><span class="n">grid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span></div>


<div class="viewcode-block" id="cuda_det"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_det.html#compas.hpc.cuda_det">[docs]</a><span class="k">def</span> <span class="nf">cuda_det</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" GPUArray square matrix determinant.</span>

<span class="sd">    Notes</span>
<span class="sd">        - Requires CULA.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray matrix of size (m x m).</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: Determinant of the square matrix.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_det(cuda_give([[5, -2, 1], [0, 3, -1], [2, 0, 7]]))</span>
<span class="sd">        103</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_dot"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_dot.html#compas.hpc.cuda_dot">[docs]</a><span class="k">def</span> <span class="nf">cuda_dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="sd">""" Matrix multiplication of two GPUArrays.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray matrix 1 (m x n).</span>
<span class="sd">        b (gpu): GPUArray matrix 2 (n x o).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: [c] = [a][b] of size (m x o)</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_give([[0, 1], [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; b = cuda_give([[0, 1], [1, 0]])</span>
<span class="sd">        &gt;&gt;&gt; c = cuda_dot(a, b)</span>
<span class="sd">        array([[ 1.,  0.],</span>
<span class="sd">               [ 3.,  2.]])</span>
<span class="sd">        &gt;&gt;&gt; type(c)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_eig"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_eig.html#compas.hpc.cuda_eig">[docs]</a><span class="k">def</span> <span class="nf">cuda_eig</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Matrix Eigenvectors and Eigenvalues of GPUArray.</span>

<span class="sd">    Notes</span>
<span class="sd">        - Requires CULA.</span>
<span class="sd">        - Input GPUArray is a square matrix, either real or complex.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray of a square matrix (m x m).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Normalised Eigenvectors (right)</span>
<span class="sd">        gpu: Eigenvalues.</span>

<span class="sd">    """</span>
    <span class="n">vr</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">vr</span><span class="p">,</span> <span class="n">w</span></div>


<div class="viewcode-block" id="cuda_hermitian"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_hermitian.html#compas.hpc.cuda_hermitian">[docs]</a><span class="k">def</span> <span class="nf">cuda_hermitian</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Hermitian conjugate transpose of GPUArray.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): Complex GPUArray.</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: The complex conjugate transpose of the GPUArray.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_hermitian(cuda_give([[1 + 2.j, 3 - 4.j],[0 - 5.j, 6 - 1.j]], type='complex'))</span>
<span class="sd">        array([[ 1.-2.j,  0.+5.j],</span>
<span class="sd">               [ 3.+4.j,  6.+1.j]], dtype=complex64)</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">hermitian</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_inv"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_inv.html#compas.hpc.cuda_inv">[docs]</a><span class="k">def</span> <span class="nf">cuda_inv</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Inverse of GPUArray matrix.</span>

<span class="sd">    Notes</span>
<span class="sd">        - Requires CULA.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): Input square GPUArray (m x m).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Matrix inverse as a GPUArray (m x m).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_inv(cuda_give([[4, 7], [2, 6]]))</span>
<span class="sd">        array([[ 0.6, -0.7],</span>
<span class="sd">               [-0.2,  0.4]])</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_normrow"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_normrow.html#compas.hpc.cuda_normrow">[docs]</a><span class="k">def</span> <span class="nf">cuda_normrow</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" GPUArray of vectors norm.2 (row by row).</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray of vectors (m x n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Vector lengths (m,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_normrow(cuda_give([[1, 2], [3, 4]]))</span>
<span class="sd">        array([ 2.23606798,  5.])</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">cuda_sqrt</span><span class="p">(</span><span class="n">cuda_sum</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span></div>


<div class="viewcode-block" id="cuda_pinv"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_pinv.html#compas.hpc.cuda_pinv">[docs]</a><span class="k">def</span> <span class="nf">cuda_pinv</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Moore-Penrose pseudo inverse of the GPUArray.</span>

<span class="sd">    Notes:</span>
<span class="sd">        - Singular values smaller than 10^-15 are set to zero.</span>
<span class="sd">        - Requires CULA.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): Input GPUArray (m x n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Pseudo inverse.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_pinv(cuda_give([[1, 3, -1], [2, 0, 3]]))</span>
<span class="sd">        array([[ 0.1056338 ,  0.16197183],</span>
<span class="sd">               [ 0.27464789,  0.02112676],</span>
<span class="sd">               [-0.07042254,  0.22535211]])</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_svd"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_svd.html#compas.hpc.cuda_svd">[docs]</a><span class="k">def</span> <span class="nf">cuda_svd</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">jobu</span><span class="o">=</span><span class="s1">'S'</span><span class="p">,</span> <span class="n">jobvt</span><span class="o">=</span><span class="s1">'S'</span><span class="p">):</span>
    <span class="sd">""" GPUArray Singular Value Decomposition.</span>

<span class="sd">    Notes</span>
<span class="sd">        - Requires CULA.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray (m x n) to decompose.</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: Unitary matrix (m x k).</span>
<span class="sd">        gpu: Singular values.</span>
<span class="sd">        gpu: vh matrix (k x n).</span>

<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_trace"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_trace.html#compas.hpc.cuda_trace">[docs]</a><span class="k">def</span> <span class="nf">cuda_trace</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" GPUArray trace, the sum along the main diagonal.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): Input GPUArray.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: tr(GPUArray).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_trace(cuda_give([[0, 1], [2, 3]]))</span>
<span class="sd">        3.0</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'numpy.float64'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<div class="viewcode-block" id="cuda_transpose"><a class="viewcode-back" href="../../../../reference/generated/compas.hpc.cuda_transpose.html#compas.hpc.cuda_transpose">[docs]</a><span class="k">def</span> <span class="nf">cuda_transpose</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="sd">""" Transpose a GPUArray.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        a (gpu): GPUArray of size (m x n).</span>

<span class="sd">    Returns:</span>
<span class="sd">        gpu: GPUArray transpose (n x m).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = cuda_transpose(cuda_give([[0, 1], [2, 3]]))</span>
<span class="sd">        array([[ 0.,  2.],</span>
<span class="sd">               [ 1.,  3.]])</span>
<span class="sd">        &gt;&gt;&gt; type(a)</span>
<span class="sd">        &lt;class 'pycuda.gpuarray.GPUArray'&gt;</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">skcuda</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<span class="c1"># ==============================================================================</span>
<span class="c1"># Debugging</span>
<span class="c1"># ==============================================================================</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>

    <span class="kn">from</span> <span class="nn">compas.numerical.hpc.gpu.cuda._array</span> <span class="k">import</span> <span class="n">cuda_give</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">cuda_transpose</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">cuda_trace</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]))</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cuda_pinv</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]))</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">cuda_normrow</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">cuda_inv</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]))</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">cuda_hermitian</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">2.</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">-</span> <span class="mf">4.</span><span class="n">j</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span> <span class="o">-</span> <span class="mf">5.</span><span class="n">j</span><span class="p">,</span> <span class="mi">6</span> <span class="o">-</span> <span class="mf">1.</span><span class="n">j</span><span class="p">]],</span> <span class="nb">type</span><span class="o">=</span><span class="s1">'complex'</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">cuda_det</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">cuda_conj</span><span class="p">(</span><span class="n">cuda_give</span><span class="p">([</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">2.</span><span class="n">j</span><span class="p">,</span> <span class="mi">3</span> <span class="o">-</span> <span class="mf">4.</span><span class="n">j</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="s1">'complex'</span><span class="p">))</span>
</code></pre></div>
<nav class="compas-sideways">
</nav>
</main>
<div class="col-12 col-md-3 col-xl-2 compas-sidebar" role="navigation">
<div class="navbar-light">
<form action="../../../../search.html" class="d-flex compas-searchbox" id="" method="get">
<input class="form-control" name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
<button aria-controls="compas-navigation" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler d-md-none compas-navigation-toggler" data-target="#compas-navigation" data-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
</form>
<div class="navbar-expand-md">
<div class="collapse navbar-collapse compas-navigation" id="compas-navigation">
<ul class="nav flex-column">
<li class="nav-item"><a class="nav-link reference internal" href="../../../../overview.html">Overview</a></li>
<li class="nav-item"><a class="nav-link reference internal" href="../../../../examples.html">Examples</a></li>
<li class="nav-item"><a class="nav-link reference internal" href="../../../../tutorial.html">Tutorial</a></li>
<li class="nav-item"><a class="nav-link reference internal" href="../../../../reference.html">Reference</a></li>
<li class="nav-item"><a class="nav-link reference internal" href="../../../../notes.html">Notes</a></li>
</ul>
</div>
</div>
</div>
</div>
<div class="d-none d-xl-block col-xl-2 compas-toc" role="toc">
</div>
</div>
</div>
<footer class="compas-footer">
            
                
                    Â© Copyright 2017, Block Research Group - ETH Zurich.
                
            

            
                Last updated on Oct 01, 2017.
            

            
                Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
            
        </footer>
<script type="text/javascript">
var DOCUMENTATION_OPTIONS = {
    URL_ROOT:    '',
    VERSION:     '',
    COLLAPSE_INDEX: false,
    FILE_SUFFIX: '.html',
    HAS_SOURCE:  true,
    SOURCELINK_SUFFIX: '.txt'
};
            </script>
<script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" src="https://code.jquery.com/jquery-3.2.1.min.js"></script>
<script crossorigin="anonymous" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"></script>
<script crossorigin="anonymous" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.0.0/anchor.js"></script>
<script src="/static/underscore.js"></script>
<script src="/static/doctools.js"></script>
<script src="/js/searchtools_.js"></script>
<script>
                hljs.initHighlightingOnLoad();
                anchors.add();
            </script>
</body>
</html>